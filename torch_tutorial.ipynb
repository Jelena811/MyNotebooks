{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "#from torchviz import make_dot Need tensorflow and some other stuff to be installed \n",
    "#bc torch doesn't have visualisation tool itself\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.utils.data.dataset import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get X and Y data created:\n",
    "np.random.seed(42)\n",
    "x = np.random.rand(100, 1)\n",
    "true_a, true_b = 1, 2\n",
    "y = true_a + true_b*x + 0.1*np.random.randn(100, 1)\n",
    "\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "#Create Custom Dataset that inherits torch's dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "#For simple datdasets use TensorDataset directly, avoid custom datasets\n",
    "dataset = TensorDataset(x_tensor, y_tensor) # dataset = CustomDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('linear.weight', tensor([[0.7645]])), ('linear.bias', tensor([0.8300]))])\n",
      "[1] Training loss: 0.326\t Validation loss: 0.122\n",
      "[2] Training loss: 0.074\t Validation loss: 0.072\n",
      "[3] Training loss: 0.056\t Validation loss: 0.060\n",
      "[4] Training loss: 0.050\t Validation loss: 0.053\n",
      "[5] Training loss: 0.045\t Validation loss: 0.047\n",
      "[6] Training loss: 0.040\t Validation loss: 0.042\n",
      "[7] Training loss: 0.036\t Validation loss: 0.038\n",
      "[8] Training loss: 0.032\t Validation loss: 0.034\n",
      "[9] Training loss: 0.029\t Validation loss: 0.031\n",
      "[10] Training loss: 0.026\t Validation loss: 0.028\n",
      "[11] Training loss: 0.024\t Validation loss: 0.026\n",
      "[12] Training loss: 0.022\t Validation loss: 0.024\n",
      "[13] Training loss: 0.020\t Validation loss: 0.022\n",
      "[14] Training loss: 0.018\t Validation loss: 0.020\n",
      "[15] Training loss: 0.017\t Validation loss: 0.019\n",
      "[16] Training loss: 0.016\t Validation loss: 0.018\n",
      "[17] Training loss: 0.015\t Validation loss: 0.016\n",
      "[18] Training loss: 0.014\t Validation loss: 0.016\n",
      "[19] Training loss: 0.013\t Validation loss: 0.015\n",
      "[20] Training loss: 0.012\t Validation loss: 0.014\n",
      "[21] Training loss: 0.012\t Validation loss: 0.013\n",
      "[22] Training loss: 0.011\t Validation loss: 0.013\n",
      "[23] Training loss: 0.011\t Validation loss: 0.013\n",
      "[24] Training loss: 0.010\t Validation loss: 0.012\n",
      "[25] Training loss: 0.010\t Validation loss: 0.012\n",
      "[26] Training loss: 0.010\t Validation loss: 0.012\n",
      "[27] Training loss: 0.009\t Validation loss: 0.011\n",
      "[28] Training loss: 0.009\t Validation loss: 0.011\n",
      "[29] Training loss: 0.009\t Validation loss: 0.011\n",
      "[30] Training loss: 0.009\t Validation loss: 0.011\n",
      "[31] Training loss: 0.009\t Validation loss: 0.011\n",
      "[32] Training loss: 0.009\t Validation loss: 0.010\n",
      "[33] Training loss: 0.009\t Validation loss: 0.010\n",
      "[34] Training loss: 0.008\t Validation loss: 0.010\n",
      "[35] Training loss: 0.008\t Validation loss: 0.010\n",
      "[36] Training loss: 0.008\t Validation loss: 0.010\n",
      "[37] Training loss: 0.008\t Validation loss: 0.010\n",
      "[38] Training loss: 0.008\t Validation loss: 0.010\n",
      "[39] Training loss: 0.008\t Validation loss: 0.010\n",
      "[40] Training loss: 0.008\t Validation loss: 0.010\n",
      "OrderedDict([('linear.weight', tensor([[1.8980]])), ('linear.bias', tensor([1.0443]))])\n"
     ]
    }
   ],
   "source": [
    "class ManualLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    def train_step(x, y):\n",
    "        model.train()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(y, yhat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    return train_step\n",
    "\n",
    "# Estimate a and b\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = ManualLinearRegression().to(device) # model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-1)\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "n_epochs = 40\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "print(model.state_dict())\n",
    "\n",
    "for epoch in range(n_epochs): #epoch prolazi jednom kroz CEO dataset\n",
    "    \n",
    "    #Za sve batch-eve do:\n",
    "    batch_losses = []\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        \n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        \n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        \n",
    "        batch_losses.append(loss)\n",
    "        \n",
    "    training_loss = np.mean(batch_losses)\n",
    "    training_losses.append(training_loss)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_losses = []\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            model.eval()\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(y_val, yhat).item()\n",
    "            val_losses.append(val_loss)\n",
    "        validation_loss = np.mean(val_losses)\n",
    "        validation_losses.append(validation_loss)\n",
    "\n",
    "    print(f\"[{epoch+1}] Training loss: {training_loss:.3f}\\t Validation loss: {validation_loss:.3f}\")\n",
    "\n",
    "print(model.state_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
